"""
Teste Completo do Pipeline RAG com Docling + Embeddings + ChromaDB

Este script testa o pipeline completo:
1. Extra√ß√£o estruturada com Docling (texto, tabelas, imagens)
2. Salvamento de imagens no SQLite (base64)
3. Chunking adaptativo
4. Gera√ß√£o de embeddings (Sentence Transformers)
5. Armazenamento no ChromaDB via LangChain
6. Query e recupera√ß√£o com imagens

Uso:
    python test_full_pipeline.py
"""

import asyncio
import logging
import os
import sys
from pathlib import Path
from datetime import datetime

# Configurar vari√°veis de ambiente antes de imports pesados
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"
os.environ["HF_HUB_DISABLE_SYMLINKS"] = "1"

# Adicionar diret√≥rio raiz do projeto ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.auth.database import init_db, SessionLocal, User, Document, DocumentImage
from src.services.ingest import process_document_with_docling
from src.services.rag import query_documents

# Configurar logging detalhado
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def print_section(title: str):
    """Imprime uma se√ß√£o formatada."""
    print("\n" + "="*80)
    print(f"  {title}")
    print("="*80 + "\n")


def setup_database():
    """Inicializa o banco de dados."""
    print_section("1. INICIALIZA√á√ÉO DO BANCO DE DADOS")

    logger.info("Criando tabelas no SQLite...")
    init_db()

    logger.info("‚úÖ Banco de dados inicializado")

    # Verificar se usu√°rio de teste existe
    db = SessionLocal()
    try:
        user = db.query(User).filter(User.id == 1).first()
        if not user:
            logger.warning("‚ö†Ô∏è  Usu√°rio de teste n√£o existe. Ser√° criado mock durante o teste.")
        else:
            logger.info(f"‚úÖ Usu√°rio encontrado: {user.user_name}")
    finally:
        db.close()


async def test_document_ingestion(pdf_path: str, force_reprocess: bool = False):
    """
    Testa o pipeline completo de ingest√£o.

    Args:
        pdf_path: Caminho do PDF
        force_reprocess: Se True, reprocessa mesmo se j√° existir

    Returns:
        doc_id do documento processado
    """
    print_section("2. INGEST√ÉO DO DOCUMENTO")

    db = SessionLocal()

    try:
        # Configurar documento de teste
        doc_id = "test_weg_guia_001"
        user_id = 1

        logger.info(f"üìÑ Arquivo: {pdf_path}")
        logger.info(f"üìã Document ID: {doc_id}")
        logger.info(f"üë§ User ID: {user_id}")

        # Verificar se arquivo existe
        if not Path(pdf_path).exists():
            logger.error(f"‚ùå Arquivo n√£o encontrado: {pdf_path}")
            return None

        # ‚úÖ VERIFICAR SE DOCUMENTO J√Å FOI PROCESSADO
        existing_doc = db.query(Document).filter(Document.id == doc_id).first()

        if existing_doc and not force_reprocess:
            logger.info(f"\n‚ôªÔ∏è  DOCUMENTO J√Å PROCESSADO - REAPROVEITANDO!")
            logger.info(f"   - Status: {existing_doc.status}")
            logger.info(f"   - Chunks: {existing_doc.chunks_count}")
            logger.info(f"   - Processado em: {existing_doc.created_at}")
            logger.info(f"   - Filename: {existing_doc.filename}")

            # Verificar imagens existentes
            images_count = db.query(DocumentImage).filter(
                DocumentImage.document_id == doc_id
            ).count()
            logger.info(f"   - Imagens salvas: {images_count}")

            logger.info("\n‚úÖ Pulando ingest√£o - usando dados existentes")
            return doc_id

        if existing_doc and force_reprocess:
            logger.warning(f"\n‚ö†Ô∏è  Documento j√° existe, mas FORCE_REPROCESS est√° ativo")
            logger.warning(f"   Removendo documento antigo e reprocessando...")

            # Remover imagens antigas
            db.query(DocumentImage).filter(
                DocumentImage.document_id == doc_id
            ).delete()

            # Remover documento antigo
            db.delete(existing_doc)
            db.commit()
            logger.info("   ‚úÖ Dados antigos removidos")

        # EXTRAIR TEXTO COMPLETO COM DOCLING E SALVAR EM TXT
        logger.info("\nüìù Extraindo texto completo com Docling...")
        from docling.document_converter import DocumentConverter

        converter = DocumentConverter()
        result = converter.convert(pdf_path)
        docling_doc = result.document

        # Salvar texto completo em arquivo
        output_dir = Path("output_extractions")
        output_dir.mkdir(exist_ok=True)

        pdf_name = Path(pdf_path).stem
        txt_file = output_dir / f"{pdf_name}_extracted_text.txt"

        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            import datetime
            f.write(f"EXTRA√á√ÉO COMPLETA DO DOCUMENTO: {Path(pdf_path).name}\n")
            f.write(f"Data: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n\n")

            # Iterar por todos os elementos e extrair texto
            for idx, (element, _) in enumerate(docling_doc.iterate_items()):
                page = element.prov[0].page_no if element.prov else 0
                el_type = element.label

                # Adicionar cabe√ßalho do elemento
                f.write(f"\n{'‚îÄ'*80}\n")
                f.write(f"[Elemento {idx+1}] P√°gina: {page} | Tipo: {el_type}\n")
                f.write(f"{'‚îÄ'*80}\n")

                # Extrair texto do elemento
                if hasattr(element, 'text') and element.text:
                    f.write(element.text)
                    f.write("\n")

                # Se for tabela, tentar extrair dados estruturados
                if el_type == "table" and hasattr(element, 'data'):
                    try:
                        table_data = element.data
                        if hasattr(table_data, 'to_string'):
                            f.write("\n[TABELA]\n")
                            f.write(table_data.to_string())
                            f.write("\n")
                    except:
                        pass

        logger.info(f"‚úÖ Texto completo salvo em: {txt_file}")
        logger.info(f"   Total de elementos processados: {len(list(docling_doc.iterate_items()))}")

        # Executar pipeline de ingest√£o
        logger.info("\nüöÄ Iniciando processamento com Docling...")
        chunks_count = await process_document_with_docling(
            file_path=pdf_path,
            doc_id=doc_id,
            user_id=user_id,
            db=db
        )

        logger.info(f"\n‚úÖ Ingest√£o conclu√≠da: {chunks_count} chunks criados")

        # Verificar imagens salvas
        images = db.query(DocumentImage).filter(
            DocumentImage.document_id == doc_id
        ).all()

        logger.info(f"\nüì∏ Imagens extra√≠das e salvas: {len(images)}")
        for i, img in enumerate(images, 1):
            logger.info(f"   {i}. P√°gina {img.page_number} - ID: {img.id}")
            logger.info(f"      Formato: {img.image_format}, Tamanho: {len(img.image_data)} bytes (base64)")
            if img.caption:
                logger.info(f"      Caption: {img.caption[:80]}...")

        return doc_id

    except Exception as e:
        logger.error(f"‚ùå Erro durante ingest√£o: {str(e)}")
        import traceback
        traceback.print_exc()
        return None
    finally:
        db.close()


async def test_embeddings_verification(user_id: int):
    """
    Verifica se os embeddings foram gerados e salvos corretamente.

    Args:
        user_id: ID do usu√°rio
    """
    print_section("3. VERIFICA√á√ÉO DE EMBEDDINGS NO CHROMADB")

    try:
        from src.services.rag import get_chroma_vectorstore

        vectorstore = get_chroma_vectorstore(user_id)

        # Tentar buscar um documento gen√©rico
        logger.info("üîç Testando busca no ChromaDB...")

        results = vectorstore.similarity_search(
            query="motor",
            k=3
        )

        if results:
            logger.info(f"‚úÖ ChromaDB est√° funcional: {len(results)} resultados encontrados")
            logger.info("\nExemplo de chunks armazenados:")
            for i, doc in enumerate(results, 1):
                logger.info(f"\n   Chunk {i}:")
                logger.info(f"   - Texto (primeiros 100 chars): {doc.page_content[:100]}...")
                logger.info(f"   - P√°gina: {doc.metadata.get('page', '?')}")
                logger.info(f"   - Tipo: {doc.metadata.get('chunk_type', '?')}")
                logger.info(f"   - Tem imagens: {doc.metadata.get('has_images', False)}")
        else:
            logger.warning("‚ö†Ô∏è  Nenhum documento encontrado no ChromaDB")

    except Exception as e:
        logger.error(f"‚ùå Erro ao verificar embeddings: {str(e)}")
        import traceback
        traceback.print_exc()


async def test_query_with_images(user_id: int):
    """
    Testa queries e recupera√ß√£o de imagens.

    Args:
        user_id: ID do usu√°rio
    """
    print_section("4. TESTE DE QUERIES E RECUPERA√á√ÉO DE IMAGENS")

    db = SessionLocal()

    try:
        # Lista de perguntas de teste
        test_questions = [
            "What are the lubrication intervals?",
            "Como fazer a manuten√ß√£o do motor?",
            "What is the NEMA frame?",
            "Quais s√£o as especifica√ß√µes t√©cnicas?",
        ]

        for i, question in enumerate(test_questions, 1):
            logger.info(f"\n{'‚îÄ'*80}")
            logger.info(f"Query {i}: '{question}'")
            logger.info('‚îÄ'*80)

            result = await query_documents(
                question=question,
                user_id=user_id,
                db=db,
                top_k=3
            )

            if result.get("error"):
                logger.error(f"‚ùå Erro: {result['error']}")
                continue

            logger.info(f"\n‚úÖ Encontrados {result['total_chunks']} chunks relevantes")

            for j, chunk in enumerate(result["chunks"], 1):
                logger.info(f"\n   üìÑ Chunk {j}:")
                logger.info(f"   - Score: {chunk['score']:.4f}")
                logger.info(f"   - P√°gina: {chunk['metadata'].get('page', '?')}")
                logger.info(f"   - Tipo: {chunk['metadata'].get('chunk_type', '?')}")
                logger.info(f"   - Texto (primeiros 150 chars):")
                logger.info(f"     {chunk['text'][:150]}...")

                if chunk["images"]:
                    logger.info(f"   - üì∏ Imagens associadas: {len(chunk['images'])}")
                    for k, img in enumerate(chunk["images"], 1):
                        logger.info(f"      {k}. ID: {img['id']}, P√°gina: {img['page']}, Formato: {img['format']}")
                        logger.info(f"         Tamanho: {len(img['data'])} bytes (base64)")
                        if img.get('caption'):
                            logger.info(f"         Caption: {img['caption'][:80]}...")
                else:
                    logger.info(f"   - üì∏ Sem imagens associadas")

        logger.info("\n" + "="*80)
        logger.info("‚úÖ Teste de queries conclu√≠do com sucesso!")
        logger.info("="*80)

    except Exception as e:
        logger.error(f"‚ùå Erro durante queries: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()


async def test_statistics():
    """Exibe estat√≠sticas do banco de dados."""
    print_section("5. ESTAT√çSTICAS DO BANCO DE DADOS")

    db = SessionLocal()

    try:
        # Contar documentos
        docs_count = db.query(Document).count()
        logger.info(f"üìÑ Total de documentos: {docs_count}")

        # Contar imagens
        images_count = db.query(DocumentImage).count()
        logger.info(f"üì∏ Total de imagens: {images_count}")

        # Listar documentos
        if docs_count > 0:
            logger.info("\nüìã Documentos no banco:")
            docs = db.query(Document).all()
            for doc in docs:
                logger.info(f"\n   - ID: {doc.id}")
                logger.info(f"     Filename: {doc.filename}")
                logger.info(f"     Status: {doc.status}")
                logger.info(f"     Chunks: {doc.chunks_count}")
                logger.info(f"     Created: {doc.created_at}")

    except Exception as e:
        logger.error(f"‚ùå Erro ao obter estat√≠sticas: {str(e)}")
    finally:
        db.close()


async def main():
    """Fun√ß√£o principal de teste."""

    print("\n" + "üöÄ"*40)
    print("  TESTE COMPLETO DO PIPELINE RAG COM DOCLING")
    print("üöÄ"*40)

    # Configura√ß√£o
    # Caminho relativo ao diret√≥rio raiz do projeto
    pdf_path = str(Path(__file__).parent.parent / "arquivo_teste" / "WEG-motores-eletricos-guia-de-especificacao-50032749-brochure-portuguese-web.pdf")
    user_id = 1

    # ‚úÖ CONTROLE DE REPROCESSAMENTO
    # Mude para True se quiser for√ßar o reprocessamento
    force_reprocess = False

    try:
        # 1. Setup
        setup_database()

        # 2. Ingest√£o (com reaproveitamento inteligente)
        doc_id = await test_document_ingestion(pdf_path, force_reprocess=force_reprocess)

        if not doc_id:
            logger.error("‚ùå Ingest√£o falhou. Abortando testes.")
            return

        # 3. Verificar embeddings
        await test_embeddings_verification(user_id)

        # 4. Testar queries
        await test_query_with_images(user_id)

        # 5. Estat√≠sticas
        await test_statistics()

        # Resumo final
        print("\n" + "‚úÖ"*40)
        print("  TODOS OS TESTES CONCLU√çDOS COM SUCESSO!")
        print("‚úÖ"*40 + "\n")

        print("üìä Resumo:")
        print(f"   ‚úÖ Documento processado: {pdf_path}")
        print(f"   ‚úÖ Embeddings gerados e salvos no ChromaDB")
        print(f"   ‚úÖ Imagens extra√≠das e salvas no SQLite")
        print(f"   ‚úÖ Queries funcionando com recupera√ß√£o de imagens")
        print(f"   ‚úÖ Pipeline completo validado!")

    except Exception as e:
        logger.error(f"\n‚ùå ERRO CR√çTICO: {str(e)}")
        import traceback
        traceback.print_exc()

        print("\n" + "‚ùå"*40)
        print("  TESTES FALHARAM!")
        print("‚ùå"*40 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
